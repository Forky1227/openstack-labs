#### 2. Create a Cinder block storage volume using the CLI

0. We'll replicate the same steps at the CLI that we just performed in Horizon. If you get the idea, and not particularly strong at the CLI, it is alright to skip this section.

0. Issue the following command to take on permissions related to the user Chester Copperpot

    `ender@controller admin >` `source chester.rc`

0. Create a new volume called **NASferatu** a **1 Gb** in size/

    `ender@controller chester >` `openstack volume create --ize 1 NASferatu`

0. Based on the output, answer the following questions:

    - **Q1: Is this a bootable volume?**
      - A1: No. The value of bootable is false.
    - **Q2: Is this volume encrypted?**
      - Q2: No. The value of encrypted is false.
    - **Q3: What is the status?**
      - Q3: The status is listed as creating

0. Issue the following command to retrieve a list of cinder volumes available to the current users chestercopperpot:

    `ender@controller chester >` `openstack volume list`
	
    - **Q1: What is teh ID of NASferatu?**
      - A1: NASferatu is identified by an UUID revealed by the openstack volume list command
    - **Q2: Has the status changed?**
      - Q2: Yes. The status is now listed as available.

0. Issue the following command to retrive a list of the instances currently in operation. Note the ID of vt2.
	
    `ender@controller chester >` `openstack server list`
	
0. Issue the following command to attach the new volume (NASferatu) to a VM instance (vt2)
	
	`[root@controller ~(keystone_chestercopperpot)]#` `nova volume-attach <replace_with_ID_of_instance_vt2> <replace_with_ID_of_volume_NASferatu> auto`
	
	* The above commands says attached NASferatu to the instance vt2, and auto assign it (alternatively you could specify /dev/vdb, /dev/vdc/, etc.)
	
	* The output should look similar to the following:
	
	```
	+----------+--------------------------------------+                                                                                                                                                                
	| Property | Value                                |                                                                                                                                                                
	+----------+--------------------------------------+                                                                                                                                                                
	| device   | /dev/vdc                             |                                                                                                                                                                
	| id       | d43ff424-684e-4aa4-953d-eb119e0c6a0d |                                                                                                                                                                
	| serverId | 5eb02b66-0e74-429a-a4d1-6c70fe18d178 |                                                                                                                                                                
	| volumeId | d43ff424-684e-4aa4-953d-eb119e0c6a0d |                                                                                                                                                                
	+----------+--------------------------------------+ 
	```

7. Check on the status of the volume (NASferatu)
	
    `[root@controller ~(keystone_chestercopperpot)]#` `cinder list`
	
	* What is the status of NASferatu?
	
8. Issue the following command to detach the volume from vt2

	`[root@controller ~(keystone_chestercopperpot)]#` `nova volume-detach <replace_with_ID_of_instance_vt2> <replace_with_ID_of_volume_NASferatu>`
	
7. Check on the status of the volume (NASferatu)
	
    `[root@controller ~(keystone_chestercopperpot)]#` `cinder list`

	* What is the status of NASferatu?

8. Now that the volume is detached, let's remove it
	
    `[root@controller ~(keystone_chestercopperpot)]#` `cinder delete NASferatu`

9. Confirm that the volume has been deleted.

    `[root@controller ~(keystone_chestercopperpot)]#`  `cinder list`
	
	* What is the state of NASferatu?
	
	* If it hasn't yet deleted, continue to issue the command until it is removed.

10. In the next section, you will SSH into vt2. You should have started it in Horizon, but take a moment and make sure vt2 is powered on (and not shutoff). You can see which VMs are powered up, or down, by using the **nova list** command. If needed, vt2 can be started with the **nova start vt2** command.
 
#### 3. Moving volumes (and data) between instances 

1.  Discover the IP address of vt2, later referenced as `<IP_ADDRESS_of_vt2>` 
 
    `nova show vt2 | grep network`

    ```
    | vault-tek-network network            | 10.10.0.4    
    ```

2. Discover the router ID which will be needed to discover network namespace ID prefixed with "qrouter-"

    `neutron router-list | cut -c -59`

    ```
    +--------------------------------------+------------------+
    | id                                   | name             |
    +--------------------------------------+------------------+
    | 5fddef2e-5cf4-4bb9-8617-57c2b9be8489 | vault-tek-router |
    +--------------------------------------+------------------+
    ```

3. SSH to neutron

    `ssh root@neutron`

    ```
    [root@neutron ~]# 
    ```

4. List the namespaces and find the "qrouter-" entry whose sufix matches the vault-tek-router's ID

    `ip netns list`

    ```
    qdhcp-20ca30a2-e3fb-4e62-bd78-08dc471e93ed
    qdhcp-d5d84797-f546-4c41-8f63-7c44954fcb8b
    qdhcp-854e1cb3-9234-43ce-b467-47d5bfdf3539
    qrouter-fe2f3cd2-9099-41d8-b231-c567b5cb4802
    qrouter-5fddef2e-5cf4-4bb9-8617-57c2b9be8489
    ```

5. SSH into vt2 via the namespace you discovered in previous step

    `ip netns exec <NAMESPACE> ssh cirros@<IP_ADDRESS_of_vt2>`

    ```
    [root@neutron ~]# ip netns exec qrouter-5fddef2e-5cf4-4bb9-8617-57c2b9be8489 ssh cirros@10.10.0.4
    cirros@10.10.0.4's password:
    ```
    > If you receive a 'no route to host' error, it is most likely because vt2 is powered off. To resolve, hop into Horizon as chestercopperpot, click on **Project > Compute > Instances** and then select **Start Instance** beside vt2.
    
6. Here is the password

    `cubswin:)`

7. Install a filesystem (assuming the drive is /dev/vdb)

    `sudo mkfs.ext4 /dev/vdb`

8. Create a directory to serve as the mount point

    `sudo mkdir -p /mnt/cargobay`

9. Mount the new directory

    `sudo mount /dev/vdb /mnt/cargobay`

10. Who are you?

    `whoami`

11. change the owner of cargo bay so you can easily add files as user = whoami

    `sudo chown -R cirros  /mnt/cargobay/`

12. Change directory into the newly mounted volume

    `cd /mnt/cargobay`

13. Add a test message to /mnt/cargobay (your present location) with the following command.

    `echo "This is a test message written to my cargo bay volume!" > test.txt`

14. Confirm that your file is there
  
    `ls -l`

15. Unmount the volume in preperation for moving it to a new instance. **(VERY IMPORANT)**

    `cd`     <-- be sure to issue this command first, otherwise you cannot unmount (can't saw off a limb you're standing on!)

    `sudo umount /mnt/cargobay`

16. Exit your SSH session to vt2

    `exit`

17. exit back to controller

    `exit`

18. Discover the vault-tek-network ID

    `neutron net-list | grep vault`

    ```
    |       ID of vault-tek-network        | Name              | subnet-ID
    ------------------------------------------------------------------------------------------------------------------
    | 854e1cb3-9234-43ce-b467-47d5bfdf3539 | vault-tek-network | 36ac8b4a-296f-4992-8248-dfccf2125da0 10.10.0.0/24    |
    ```

19. Start a new instance (vt3... whatever you want). Use the same security-group (http-ssh) and network as vt2. Here is a sample if you want to try booting the new instance from the command line. Be sure the edit the net-id below because this one is **just an example**.

    `nova boot --flavor m1.tiny --image cirros --nic net-id=<ID of vault-tek-network in the previous step> --security-groups http-ssh   vt3`

20. Want to do a little practice?  Delete the above VM and start it again on your own, both from the dashboard and also from the command line. WARNING, ONLY two machines will run and one time!!!

21. Discover instance IDs

    `nova list`

    ```
    +--------------------------------------+------+---------+------------+-------------+-----------------------------+
    | ID                                   | Name | Status  | Task State | Power State | Networks                    |
    +--------------------------------------+------+---------+------------+-------------+-----------------------------+
    | fd70c522-28da-4607-9610-af766440ac1a | vt2  | SHUTOFF | -          | Shutdown    | vault-tek-network=10.10.0.4 |
    | 1accbf8d-0d0c-43be-b314-4f8d45672929 | vt3  | ACTIVE  | -          | Running     | vault-tek-network=10.10.0.5 |
    +--------------------------------------+------+---------+------------+-------------+-----------------------------+
    ```

22. Discover cinder volumes

    `cinder list`

    ```
    +--------------------------------------+--------+--------------+------+-------------+----------+--------------------------------------+
    |                  ID                  | Status | Display Name | Size | Volume Type | Bootable |             Attached to              |
    +--------------------------------------+--------+--------------+------+-------------+----------+--------------------------------------+
    | b9f00354-0df1-4d5f-bc23-6423478cf334 | in-use |  cargo-bay   |  1   |    iscsi    |  false   | fd70c522-28da-4607-9610-af766440ac1a |
    +--------------------------------------+--------+--------------+------+-------------+----------+--------------------------------------+
    ```

23. Here is a nice way to discover where this volume is connected in your instance! 

    `cinder show cargo-bay | grep /dev/`  

    > Most likely the volume will be connected to: `/dev/vdb` 


24. Detach cargo-bay

    `nova volume-detach <vt2-ID> <cargo-bay_ID>`

25. Confirm detachment  (Wait until status changes from "detaching" to "Available"

    `cinder list`

    ```
    +--------------------------------------+-----------+--------------+------+-------------+----------+--------------------------------------+
    |                  ID                  |   Status  | Display Name | Size | Volume Type | Bootable |             Attached to              |
    +--------------------------------------+-----------+--------------+------+-------------+----------+--------------------------------------+
    | b9f00354-0df1-4d5f-bc23-6423478cf334 | detaching |  cargo-bay   |  1   |    iscsi    |  false   | fd70c522-28da-4607-9610-af766440ac1a |
    +--------------------------------------+-----------+--------------+------+-------------+----------+--------------------------------------+
    ```

25. The above status says 'detaching'; status needs to read 'available' before you can continue to attach the volume cargo-bay to the new instance vt3.

    `nova volume-attach <vt3-ID> <cargo-bay-ID> auto`

26. SSH into vt3 (reference steps 1 to 6 for help) 

27. Mount cargo bay (reference steps 8 to 12 for help) <-- IMPORTANT !!!DO NOT REPERFORM STEP 7 OR YOU WILL DELETE YOUR TEXT FILE!!!

28. Look at the new volume on vt3 and confirm it contains the test file that you created.
