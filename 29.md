---
date: "2016-10-09"
draft: false
weight: 290
title: "Lab 29 - Creating Block Storage Volumes with Cinder at the CLI"
---
[Click here to find out more about Alta3 Research's Openstack Training](https://alta3.com/courses/openstack)

### THURSDAY- &#x2B50;REQUIRED&#x2B50;

### Lab Objective 

The objective of this lab is to create a block storage volume with Cinder at the CLI, attach it to a VM instance, create some data on that volume, detach that volume, attach it to a new VM, and then confirm that the data moved with the cinder volume.

### Procedure

0. We don't need any old terminal windows open, so close them all, and then open a new one. Once you have a new terminal window open, SSH to controller. 

    `student@beachhead:/$` `ssh controller`

0. The CLI prompt should now read as follows:

    `student@controller:~$`

0. Source the chester.rc permissions file

    `student@controller:~$` `source chester.rc`

0. Create a new volume called **NASferatu** a **1 Gb** in size/

    `student@controller (chester) :~$` `openstack volume create --size 1 NASferatu`

    ![Create 1 Gb Volume](https://alta3.com/labs/images/alta3_lab_volume_create.png)

0. Based on the output, answer the following questions:

    - **Q1: Is this a bootable volume?**
      - A1: No. The value of bootable is false.
    - **Q2: Is this volume encrypted?**
      - Q2: No. The value of encrypted is false.
    - **Q3: What is the status?**
      - Q3: The status is listed as creating

0. Issue the following command to retrieve a list of cinder volumes available to the current users chestercopperpot:

    `student@controller (chester) :~$` `openstack volume list`
    
    ```
    +--------------------------------------+--------------+-----------+------+------------------------------+
    | ID                                   | Display Name | Status    | Size | Attached to                  |
    +--------------------------------------+--------------+-----------+------+------------------------------+
    | 857334ec-92ce-43dc-83b5-b801b77337c6 | NASferatu    | available |    1 |                              |
    | d37ab4c8-786e-4915-a94c-0c5ff5e3c20c | cargo-bay    | in-use    |    1 | Attached to vt1 on /dev/vdb  |
    +--------------------------------------+--------------+-----------+------+------------------------------+
    ```

0. Based on the output from the previous command, *openstack volume list*, answer the following questions.
	
    - **Q1: What is the ID of NASferatu?**
      - A1: NASferatu is identified by an UUID revealed by the openstack volume list command
    - **Q2: Has the status changed?**
      - Q2: Yes. The status is now listed as available.

0. Issue the following command to retrive a list of the instances currently in operation. Note the ID of vt2.
	
    `student@controller (chester) :~$` `openstack server list`
	
0. Issue the following command to attach the new volume (NASferatu) to a VM instance (vt2)
	
    `student@controller (chester) :~$` `openstack server add volume <replace_with_ID_of_instance_vt2> <replace_with_ID_of_volume_NASferatu>`
	
0. Confirm that NASferatu has been Attached to vt2
	
    `ender@controller chester >` `openstack volume list`
	
0. Issue the following command to detach the volume from vt2

    `ender@controller chester >` `openstack server remove volume <replace_with_ID_of_instance_vt2> <replace_with_ID_of_volume_NASferatu>`
	
0. Check on the status of the volume (NASferatu)
	
    `ender@controller chester >` `openstack volume list`

    - **Q1: What is the status of NASferatu?**
      - A1: It has changed from in-use, to available.

0. Now that the volume is detached, delete it.
	
    `ender@controller chester >` `openstack volume delete <replace_with_ID_of_volume_NASferatu>`

0. Confirm that the volume has been deleted.

    `ender@controller chester >`  `openstack volume list`
	
    - **Q1: What is the status of NASferatu?**
      - A1: NASferatu has been deleted and is no longer available.

0. In the next section, you will SSH into vt2. You should have started it in Horizon, but take a moment and make sure vt2 is powered on (and not shutoff). You can see which VMs are powered up, or down, by using the **nova list** command. If needed, vt2 can be started with the **nova start vt2** command.
